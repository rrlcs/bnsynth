\section{Future work and extensions}

\noindent We wish to achieve the following in the coming weeks:

\begin{itemize}
\item Complete the implementation of SyGuS-Sketcher and compare it emprically with DeepSynth.
\item Generate dataset as explained and train the proposed NN model for invariant benchmarks and perform relevant experimentation.
\item Extend to more classes of SyGuS benchmarks such as CLIA to check if this architecture can fill the gaps, as mentioned in the motivating example.
\end{itemize} 
\smallskip
\noindent\textbf{Research directions: }

If a sketch generated by the neural network is infeasible (i.e. the solver is not able to find a valid completion of the sketch that satisfies the specification), how do we communicate it back to the neural network? More specifically, how do we leverage a neural network to understand infeasible sketches, given an embedding of a bad sketch using a GGNN?  A deduction guided RL approach \cite{chenprogram} uses a similar idea but to update the policy of an RL network when an infeasible partial program is returned.
 
The verifier that SyGuS-Sketcher uses can only synthesize constant holes. Consequently, infeasible sketches can be encoded as logical formulae with constant literals and fed as a constraint back to the synthesizer. However, if the hole is not a constant hole, how do we regard it as a bad sketch is still not clear. One plausible direction is to look into the use of Conflict Driven Learning techniques for the same.