\section{Future Directions}
\noindent Following is a consolidation of the discussions held in previous meetings.

\subsection{Sampling Strategy}
\noindent\textbf{Seed based Sampling:}
Instead of going for Random Sampling, we can use a few seed examples from the SAT solver. 
Once we have the seed examples, we can sample around these examples to get the training data.
This would give more accurate training data faster.

\smallskip
\noindent\textbf{Simulated Annealing:} 
This is another way to avoid Random Sampling. Simulated Annealing would be faster and 
won't require the seed examples as well. This method is based on Energy Functions.

\subsection{Training}
\noindent\textbf{Counter Example Guided Training:} If the first output of the model doesn't give us the 
intended result, we can generate countere examples using a SAT solver and feed that back to the Network
 through a feedback loop. This will tell exactly where the model is failing.

 \smallskip
 \noindent\textbf{Loss Function:} Instead of computing the loss over model's numerical output, 
 can we compute loss over final formula extracted from the network? 

 \noindent As we don't have final formulae in our dataset, this idea doesn't seem to work as of now.

 \smallskip
\noindent\textbf{Hyperparameter Search:} Instead of trying out different hyperparameter values manually, 
we can automate this process. An efficient way to do this is through Hyperparameter Search.