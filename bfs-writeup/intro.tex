% !Tex root=main.tex

\section{Introduction}
\label{sec:intro}

Recent years have seen a surge of interest in the area of program synthesis where generating correct-by-construction programs with respect to high-level specifications is the end goal. In the current decade, these techniques have been proven to be beneficial to end users which was earlier not possible, especially during the nascent years of this area in the 1950's. Recently, there has been a lot of work in the intersection of Machine Learning with Program Synthesis and this is the milieu for our report.

Although there are many techniques in this area, we consider the sub-area of programming by sketches  which is a Good Old Fashioned Program Synthesis (GOFPS) technique i.e.  classical AI techniques that do not consider statistical methods.  In this report, we focus on the automation of sketch generation using deep learning techniques.

Sketches are partial programs that aid the process of program synthesis from specifications. They restrict the search space of programs for the synthesizer thereby speeding up the synthesis process. However, the speed-up is contingent on the fact that we synthesize the right sketch for the input specification. Intuitively, a  right sketch is a partial program for which there exists at least one solution i.e a complete program that satisfies the specification for all inputs, obtained by completing the sketch.

Sketches were first introduced in \cite{10.5555/1714168}. In this work, a user specifies a sketch which is a partial program. and a specification that constrains the behaviour of the completed sketch. The sketch synthesizer then converts the sketch and specification into a boolean formula which is fed into a SAT solver that completes the sketch to satisfy the specification. This work is the first to describe the Counter Example Guided Inductive Synthesis (CEGIS) architecture and is used to synthesize holes that are constants. Here, the user is expected to provide the Sketches. Providing sketches as well as the specification may be inefficient as the user may perform redundant work. Moreover, it may be infeasible to always generate a sketch manually. It is thus beneficial to generate the right sketch automatically with respect to a specification.

SketchAdapt \cite{nye2019learning} learns to infer sketches automatically using statistical techniques. This work is inspired from RobustFill \cite{devlin2017robustfill} and Deepcoder \cite{balog2016deepcoder}.
The key idea here is to generate program sketches from user specifications i.e. I/O examples or natural language, that flexibly manage the work load between neural synthesis and symbolic search. Sketches are essentially valid program trees in the DSL, where any number of sub-trees have been replaced by a special token called $HOLE$. Intuitively, this token designates locations in the program tree for which pattern-based recognition is difficult, and more explicit search methods are necessary. This tool can be divided into two components: 1) \emph{Neural Sketch Generator} which is a seq2seq neural architecture. It takes the spec. as input sequence and produces a good sketch as a sequence of tokens, and 2) \emph{Program Synthesizer} which takes the generated sketch and outputs a final program by choosing grammar productions based on learned production probabilities. Generating a good sketch prunes out a large part of program space, thereby improving the synthesis time. Due to use of symbolic search to fill up the sketches, programs produced are more generic. While this work is a huge success for the domains of list processing and string processing, it does not work for logical formulae. Further, it requires millions of training data for training the network which is not readily available for logical domains. Moreover, I/O specifications are finite and do not capture an infinite state space. Thus, synthesizing sketches for logical specifications remain an open problem.

The closest approach for synthesizing sketches for logical specifications is that of Counter Example Guided Inductive Synthesis modulo Theories i.e CEGIS(T) \cite{abate2018counterexample}. In this work, the solver invokes CEGIS to synthesize a sketch (or a program skeleton as they call it). A sketch in this case is a program synthesized by constraint solving or enumerative solving techniques where the constants are replaced with holes. The sketch is then fed into a constraint solver to check feasibility. If feasible, the constraint solver figures out the right constants and generates the final program. Otherwise, the sketch will not have any valid completion that satisfy the logical specification. In this case, the "bad" sketch is appended as additional information to the original constraint. The synthesizer then uses this information to prune out the space of sketches. Their tool FastSynth implements this approach. Although their sketches can be synthesized from logical specifications, the sketches obtained are not really sketches in the true sense because they are essentially synthesized programs where constants are replaced with holes. We wish to develop a tool that synthesizes sketches (directly from logical specifications) dynamically when pattern based techniques are unable to proceed further.

Counter Example Guided Neural Synthesis (CEGNS) \cite{polgreen2020counterexample} extends the CEGIS loop of FastSynth to synthesize programs in the counterexample loop using a neural network. In this case, the neural network does not take as input an embedding of the logical specification but a set of I/O examples that are sampled from this logical specification. Also, this tool synthesizes complete programs instead of sketches which are verified by an out of the box verifier. Their tool DeepSynth implements this approach.  However, the results are not impressive and it would not be surprising if a better model exists to improve upon this tool.  Moreover, the neural network still takes as input an incomplete specification in the form of I/O examples and not a complete specification such as logical formulae. Thus, it is natural to ask the question whether a neural network can understand  logical specifications well enough to synthesize sketches or complete programs.

Another work which is close to our idea is the Multi-modal Synthesis of Regular Expressions \cite{chen2019multi} where they synthesize regular expressions from two modes of specifications i.e. natural language and I/O examples respectively. They first use a semantic parser to generate from English descriptions a type of sketch which they call a hierarchical sketch (say S). Given S, their Programming By Examples (PBE) engine synthesizes a regular expression that is a valid completion of S while satisfying the I/O specification. Although this work shows promise, it is likely that the user intent may be misrepresented due to the fact that natural language can be inherently ambiguous. Moreover, these specification do not capture an infinite space of examples. Thus, being able to use logical formulae to aid the process of sketch synthesis is of importance. We aim to use such a multi-modal synthesis technique but using statistical methods for logical specifications.

Thus,  as discussed,  we are curious to understand \emph{if a model can understand the syntax (and semantics) of a logical specification to faithfully reproduce a program (possibly partial) that satisfies the specification}.  A related work \cite{evans2018neural} in this area investigates if a neural network can understand the task of logical entailment. They mention that logical entailment is a well suited task to check whether a neural network can understand structure which is imperative for many language and reasoning tasks.  They validate the same with empirical evidence.

Recent works \cite{glorot2019learning} with Graph Neural Networks have shown significant improvement in such tasks. Thus,  we may also ask if GNN's can serve to understand the semantics of logical constraints to synthesize a sketch.

\noindent Based on this exhaustive survey, we propose the following problem statement:

\noindent\textbf{Problem Statement:} Given a logical specification and a DSL for the space of programs, we wish to design a system that synthesizes sketches in such a way that the work load between neural synthesis and symbolic search is managed efficiently i.e. we want a neural network to synthesize sketches when pattern based techniques are unable to proceed and invoke a solver to complete the sketch. We want the neural network to be trained using a multi-modal "concolic" approach i.e by using an input embedding based on logical specifications and I/O examples generated from these I/O examples.

To the best of our knowledge, we are the first to work on sketch generation using logical embeddings.

\smallskip
\noindent\textbf{Other works using sketches: } 
Neural Edit Completion \cite{brody2020neural} is a code completion technique that uses the context of the code environment as specification. A sketch in this context is the partial code that the user provides.

%Compare Sketches with code completion (predicts code) by Eran Yahav
%and edit completion (predicts edits)2020 Yahav
%Spec is context here!! Based on context you can predict a lot. Not keen on writing specs in FOL for sorting an array but ...(see the video) 45 mins approx
Program synthesis using conflict-driven learning \cite{feng2018program} also talks about sketches in terms of partial programs. 

\smallskip
\noindent\textbf{Contribution: } In this report, we will talk about a research proposal and preliminary work in this direction. The major topics are:
\begin{itemize}
\item  A line of direction to generate synthetic dataset for SyGuS benchmarks.
\item  A "concolic" neural network architecture using logical embeddings and I/O embeddings.
\item  A prototype tool SyGuS-Sketcher using existing techniques with experimentation plan.
\end{itemize}